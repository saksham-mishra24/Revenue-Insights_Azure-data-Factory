# Data Ingestion :

The data ingestion phase is a crucial step in the project, where the hospitality data is obtained in CSV format and ingested into Azure Data Factory (ADF) for further processing.

### This section outlines the steps involved in the data ingestion process.

## Steps for Data Ingestion:

## :one: Obtaining the Data:

* Obtain the hospitality data from the client in CSV format.

* Ensure that the data is properly formatted and adheres to the expected schema.

## :two: Setting up Linked Services:

* Create the necessary linked services in Azure Data Factory to establish connections to the source system and Blob Storage.

* Configure the linked service for the source system, providing the required credentials and connection details to access the CSV files.

* Set up the linked service for Blob Storage, specifying the storage account details and authentication method.

## :three: Creating Datasets:

* Define the datasets within Azure Data Factory to represent the source CSV files and the Blob Storage container.
* Configure the source dataset to reference the linked service for the source system and specify the file format, delimiter, and column mappings.
* Set up the dataset for the Blob Storage sink, associating it with the appropriate linked service and specifying the target folder or container structure.

## :four: Setting up the ADF Pipeline:

* Create an Azure Data Factory (ADF) pipeline to orchestrate the data ingestion process.
* Configure the necessary connections and resources, including the source CSV files and the Blob Storage account, within the ADF pipeline.
* Setting up the ADF Pipeline:

## :five: Configuring the Source:

* Define the source dataset within the ADF pipeline, specifying the location and format of the CSV files in the source system.

* Configure the source dataset properties such as file format, delimiter, and column mappings to ensure accurate data extraction.

* Configure the necessary activities, using the source dataset and linked service to read the CSV files and the Blob Storage dataset and linked service to write the ingested data.

## :six: Storing the Data in Blob Storage:

* Configure a Blob Storage sink within the ADF pipeline to store the ingested data.

* Define the target folder or container structure within Blob Storage to efficiently organize the data.

## :seven: Scheduling and Monitoring:

* Set up a scheduling mechanism within ADF to automate the data ingestion process at specified intervals or based on specific triggers.

* Enable monitoring and logging features to track the execution status and identify any issues during the data ingestion process.

## :jack_o_lantern: Expected Output:

* By following these steps, the data ingestion phase of the project achieves the following:

###  - Successful acquisition of hospitality data in CSV format from the client.
###  - Proper configuration and setup of the ADF pipeline for data ingestion.
###  - Storage of the ingested data in the designated Blob Storage container or folder structure.
###  - Automated scheduling and monitoring capabilities to streamline the data ingestion process.
