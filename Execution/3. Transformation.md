# Data Transformation using Data Flow

### This section outlines the steps involved in the data transformation process using Data Flow.

## Steps for Data Transformation:

### :one:. Setting up Data Flow:

* - Create a Data Flow activity within the ADF pipeline to perform advanced data transformations and processing.
* - Define the necessary connections and resources required for Data Flow, including the source dataset and the sink dataset.
### :two: Configuring Source and Sink:

* - Configure the source dataset within the Data Flow, specifying the input data source (e.g., CSV files in Blob Storage) and the associated schema.
* - Define the sink dataset, indicating the output data destination (e.g., ADLS Gen2) and the schema for the transformed data.
### :three: Data Transformation Steps:

* - Utilize various Data Flow transformations to clean, transform, and enrich the data. Here are some commonly used transformations:
* - Derived Column: Create new columns or modify existing columns based on custom expressions.
* - Aggregate: Perform aggregations on specific columns to calculate metrics such as revenue, total bookings, occupancy percentage, average rating, etc.
* - Union: Combine multiple datasets or branches of the data flow into a single dataset.
* - Select: Choose specific columns or remove unwanted columns from the dataset.
* - Conditional Split: Split the data into different paths based on conditional expressions.
* - Filter: Apply filters to exclude or include specific rows based on defined criteria.
* - Join: Merge two or more datasets based on common columns.
* - Lookup: Perform lookup operations to enrich the data with additional information from reference datasets.
### :four: Applying Measures:

* - Implement the defined measures (e.g., Revenue, Total Bookings, Occupancy %, Average Rating) using appropriate transformations within the Data Flow.
* - Use aggregations, derived columns, and other transformations to calculate the measures based on the business requirements.
### :five: Data Sink:

* - Connect the output of the Data Flow to the sink dataset, specifying the location and format of the transformed data in the destination (e.g., ADLS Gen2).
* - Define the schema and mapping between the transformed data and the sink dataset.
### :six: Validating and Debugging:

* - Validate the Data Flow by previewing the transformed data and ensuring that the transformations are applied correctly.
* - Debug the Data Flow if needed by analyzing any errors or unexpected results during the transformation process.
### :seven: Publishing and Executing the Pipeline:

* - Publish the changes made in the Data Flow to make them available for execution.
* - Execute the ADF pipeline to trigger the Data Flow activity and perform the data transformation process.

## Expected Output:
### By following these steps, the data transformation phase of the project achieves the following:

* - Configuration and setup of the Data Flow activity within the ADF pipeline for advanced data transformations.
* - Proper connection and configuration of the source and sink datasets within the Data Flow.
* - Application of various Data Flow transformations to clean, transform, and enrich the data.
* - Implementation of the defined measures using appropriate transformations.
* - Successful mapping and writing of the transformed data to the sink dataset.
* - Validation and debugging of the Data Flow to ensure accuracy and reliability.
* - Execution of the ADF pipeline to trigger the Data Flow activity and perform the data transformation process.
